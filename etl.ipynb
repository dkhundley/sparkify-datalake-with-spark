{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify - Data Lake with Spark\n",
    "In this notebook, we'll be testing out migrating JSON data hosted in Udacity's Sparkify S3 buckets parquet files hosted in our own S3 bucket. In this test notebook, we will demonstrate that this all works appropriately prior to utilizing the information within etl.py directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup\n",
    "Getting everything together that we'll need for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages from etl.py template file\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:2.7.0') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing AWS credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['KEYS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['KEYS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating variables pointing to where data is stored\n",
    "input_data = 's3a://udacity-dend/'\n",
    "output_data = 'aws_data/'\n",
    "\n",
    "song_data = input_data + 'song_data/*/*/*/*.json'\n",
    "log_data = input_data + 'log_data/*/*/*.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting song data\n",
    "with zipfile.ZipFile(song_data, 'r') as song_zip:\n",
    "    song_zip.extractall('data/song_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting log data\n",
    "with zipfile.ZipFile(log_data, 'r') as log_zip:\n",
    "    log_zip.extractall('data/log_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Song Data\n",
    "In this section, we'll process through the steps to appropriate populate our dimension tables from the song data provided by Udacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the song data into the 'song_df' DataFrame\n",
    "song_df = spark.read.json('data/song_data/song_data/A/*/*/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing the schema of the newly imported song_df\n",
    "song_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist_id='ARDR4AC1187FB371A1', artist_latitude=None, artist_location='', artist_longitude=None, artist_name='Montserrat Caball√©;Placido Domingo;Vicente Sardinero;Judith Blegen;Sherrill Milnes;Georg Solti', duration=511.16363, num_songs=1, song_id='SOBAYLL12A8C138AF9', title='Sono andati? Fingevo di dormire', year=0),\n",
       " Row(artist_id='AREBBGV1187FB523D2', artist_latitude=None, artist_location='Houston, TX', artist_longitude=None, artist_name=\"Mike Jones (Featuring CJ_ Mello & Lil' Bran)\", duration=173.66159, num_songs=1, song_id='SOOLYAZ12A6701F4A6', title='Laws Patrolling (Album Version)', year=0),\n",
       " Row(artist_id='ARMAC4T1187FB3FA4C', artist_latitude=40.82624, artist_location='Morris Plains, NJ', artist_longitude=-74.47995, artist_name='The Dillinger Escape Plan', duration=207.77751, num_songs=1, song_id='SOBBUGU12A8C13E95D', title='Setting Fire to Sleeping Giants', year=2004),\n",
       " Row(artist_id='ARPBNLO1187FB3D52F', artist_latitude=40.71455, artist_location='New York, NY', artist_longitude=-74.00712, artist_name='Tiny Tim', duration=43.36281, num_songs=1, song_id='SOAOIBZ12AB01815BE', title='I Hold Your Hand In Mine [Live At Royal Albert Hall]', year=2000),\n",
       " Row(artist_id='ARDNS031187B9924F0', artist_latitude=32.67828, artist_location='Georgia', artist_longitude=-83.22295, artist_name='Tim Wilson', duration=186.48771, num_songs=1, song_id='SONYPOM12A8C13B2D7', title='I Think My Wife Is Running Around On Me (Taco Hell)', year=2005)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing first few rows of song_df\n",
    "song_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Table\n",
    "We'll peel off the columns needed for this specific table and write them to the approproriate parquet file, partitioned by year and artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peeling off appropriate columns from song_df for songs_table\n",
    "songs_table = song_df['song_id', 'title', 'artist_id', 'year', 'duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning by year & artist_id and writing to the parquet file\n",
    "songs_table.write.partitionBy('year', 'artist_id').parquet(output_data + 'songs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artist Table\n",
    "We'll peel off the columns needed for this specific table and write them to the appropriate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peeling off appripriate columns from song_df for the artists_table\n",
    "artists_table = song_df['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the artists_table information to the appropriate parquet file\n",
    "artists_table.write.parquet(output_data + 'artists/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Data\n",
    "Following a similar approach as the \"Song Data\" section, we'll use the log data to appropriately populate our fact table and respective dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the log data into the 'log_df' DataFrame\n",
    "log_df = spark.read.json('data/log_data/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing the schema of the newly imported log_df\n",
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Harmonia', auth='Logged In', firstName='Ryan', gender='M', itemInSession=0, lastName='Smith', length=655.77751, level='free', location='San Jose-Sunnyvale-Santa Clara, CA', method='PUT', page='NextSong', registration=1541016707796.0, sessionId=583, song='Sehr kosmisch', status=200, ts=1542241826796, userAgent='\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/36.0.1985.125 Chrome/36.0.1985.125 Safari/537.36\"', userId='26'),\n",
       " Row(artist='The Prodigy', auth='Logged In', firstName='Ryan', gender='M', itemInSession=1, lastName='Smith', length=260.07465, level='free', location='San Jose-Sunnyvale-Santa Clara, CA', method='PUT', page='NextSong', registration=1541016707796.0, sessionId=583, song='The Big Gundown', status=200, ts=1542242481796, userAgent='\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/36.0.1985.125 Chrome/36.0.1985.125 Safari/537.36\"', userId='26'),\n",
       " Row(artist='Train', auth='Logged In', firstName='Ryan', gender='M', itemInSession=2, lastName='Smith', length=205.45261, level='free', location='San Jose-Sunnyvale-Santa Clara, CA', method='PUT', page='NextSong', registration=1541016707796.0, sessionId=583, song='Marry Me', status=200, ts=1542242741796, userAgent='\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/36.0.1985.125 Chrome/36.0.1985.125 Safari/537.36\"', userId='26'),\n",
       " Row(artist=None, auth='Logged In', firstName='Wyatt', gender='M', itemInSession=0, lastName='Scott', length=None, level='free', location='Eureka-Arcata-Fortuna, CA', method='GET', page='Home', registration=1540872073796.0, sessionId=563, song=None, status=200, ts=1542247071796, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko', userId='9'),\n",
       " Row(artist=None, auth='Logged In', firstName='Austin', gender='M', itemInSession=0, lastName='Rosales', length=None, level='free', location='New York-Newark-Jersey City, NY-NJ-PA', method='GET', page='Home', registration=1541059521796.0, sessionId=521, song=None, status=200, ts=1542252577796, userAgent='Mozilla/5.0 (Windows NT 6.1; rv:31.0) Gecko/20100101 Firefox/31.0', userId='12')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing first few rows of log_df\n",
    "log_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Table\n",
    "We'll peel off the columns needed for this specific table and write them to the appropriate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peeling off appripriate columns from log_df for the users_table\n",
    "users_table = log_df['userId', 'firstName', 'lastName', 'gender', 'level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the users_table information to the appropriate parquet file\n",
    "users_table.write.parquet(output_data + 'users/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Table\n",
    "This table is going to be a bit tricky because while we are given the timestamp, we are not provided with the individual fields called for by the project. No matter, we'll simply extract this appropriate information from the timestamp ('ts') field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a UDF to extract the information needed appropriately\n",
    "get_datetime = udf(lambda x: datetime.fromtimestamp(x / 1000.0).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new 'start_date' column using UDF defined above\n",
    "log_df = log_df.withColumn('start_date', get_datetime(log_df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating time_table using PySpark SQL functions to extract information appropriately\n",
    "time_table = log_df.select('start_date',\n",
    "                           hour('start_date').alias('hour'),\n",
    "                           dayofmonth('start_date').alias('day'),\n",
    "                           weekofyear('start_date').alias('week'),\n",
    "                           month('start_date').alias('month'),\n",
    "                           year('start_date').alias('year'),\n",
    "                           dayofweek('start_date').alias('weekday')\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the time_table information to the appropriate parquet file and partitioning by year/month\n",
    "time_table.write.partitionBy('year', 'month').parquet(output_data + 'time/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Songplays Table\n",
    "Being the fact table, creating this table will be a tad tricky as we'll need to leverage some of the data we already worked with above. Let's first read back in those parquet files we established above for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in parquet file from above\n",
    "songs_table_df = spark.read.parquet(output_data + '/songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining all the tables together\n",
    "log_df = log_df.join(songs_table_df, (log_df.song == songs_table_df.title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating baseline songplay_table WITHOUT songplay_id\n",
    "songplay_table = log_df['start_date', 'userId', 'level', 'song_id', 'artist_id', 'location', 'userAgent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Special thanks to this post for helping me figure out how to add songplay_id column: https://stackoverflow.com/questions/32086578/how-to-add-row-id-in-pyspark-dataframes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding songplay_id column\n",
    "songplay_table = songplay_table.withColumn('songplay_id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing songplay_table to appropriate parquet file\n",
    "songplay_table.write.parquet(output_data + 'songplay/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

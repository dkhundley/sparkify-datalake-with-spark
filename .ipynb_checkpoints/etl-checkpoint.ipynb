{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify - Data Lake with Spark\n",
    "In this notebook, we'll be testing out migrating JSON data hosted in Udacity's Sparkify S3 buckets parquet files hosted in our own S3 bucket. In this test notebook, we will demonstrate that this all works appropriately prior to utilizing the information within etl.py directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup\n",
    "Getting everything together that we'll need for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages from etl.py template file\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:2.7.0') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing AWS credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['KEYS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['KEYS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating variables pointing to where data is stored\n",
    "input_data = 's3a://udacity-dend/'\n",
    "output_data = 'aws_data/'\n",
    "\n",
    "song_data = input_data + 'song_data/*/*/*/*.json'\n",
    "log_data = input_data + 'log_data/*/*/*.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Song Data\n",
    "In this section, we'll process through the steps to appropriate populate our dimension tables from the song data provided by Udacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the song data into the 'song_df' DataFrame\n",
    "song_df = spark.read.json('data/song-data/song_data/A/*/*/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing the schema of the newly imported song_df\n",
    "song_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist_id='ARDR4AC1187FB371A1', artist_latitude=None, artist_location='', artist_longitude=None, artist_name='Montserrat Caball√©;Placido Domingo;Vicente Sardinero;Judith Blegen;Sherrill Milnes;Georg Solti', duration=511.16363, num_songs=1, song_id='SOBAYLL12A8C138AF9', title='Sono andati? Fingevo di dormire', year=0),\n",
       " Row(artist_id='AREBBGV1187FB523D2', artist_latitude=None, artist_location='Houston, TX', artist_longitude=None, artist_name=\"Mike Jones (Featuring CJ_ Mello & Lil' Bran)\", duration=173.66159, num_songs=1, song_id='SOOLYAZ12A6701F4A6', title='Laws Patrolling (Album Version)', year=0),\n",
       " Row(artist_id='ARMAC4T1187FB3FA4C', artist_latitude=40.82624, artist_location='Morris Plains, NJ', artist_longitude=-74.47995, artist_name='The Dillinger Escape Plan', duration=207.77751, num_songs=1, song_id='SOBBUGU12A8C13E95D', title='Setting Fire to Sleeping Giants', year=2004),\n",
       " Row(artist_id='ARPBNLO1187FB3D52F', artist_latitude=40.71455, artist_location='New York, NY', artist_longitude=-74.00712, artist_name='Tiny Tim', duration=43.36281, num_songs=1, song_id='SOAOIBZ12AB01815BE', title='I Hold Your Hand In Mine [Live At Royal Albert Hall]', year=2000),\n",
       " Row(artist_id='ARDNS031187B9924F0', artist_latitude=32.67828, artist_location='Georgia', artist_longitude=-83.22295, artist_name='Tim Wilson', duration=186.48771, num_songs=1, song_id='SONYPOM12A8C13B2D7', title='I Think My Wife Is Running Around On Me (Taco Hell)', year=2005)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing first few rows of song_df\n",
    "song_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Table\n",
    "We'll peel off the columns needed for this specific table and write them to the approproriate parquet file, partitioned by year and artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peeling off appropriate columns from song_df for songs_table\n",
    "songs_table = song_df['song_id', 'title', 'artist_id', 'year', 'duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates from the table\n",
    "songs_table = songs_table.drop_duplicates(subset=['song_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning by year & artist_id and writing to the parquet file\n",
    "songs_table.write.partitionBy('year', 'artist_id').parquet(output_data + 'songs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artist Table\n",
    "We'll peel off the columns needed for this specific table and write them to the appropriate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peeling off appripriate columns from song_df for the artists_table\n",
    "artists_table = song_df['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates from the table\n",
    "artists_table = artists_table.drop_duplicates(subset=['artist_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the artists_table information to the appropriate parquet file\n",
    "artists_table.write.parquet(output_data + 'artists/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Data\n",
    "Following a similar approach as the \"Song Data\" section, we'll use the log data to appropriately populate our fact table and respective dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the log data into the 'log_df' DataFrame\n",
    "log_df = spark.read.json('data/log-data/*.json').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing the schema of the newly imported log_df\n",
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Fat Joe', auth='Logged In', firstName='Kate', gender='F', itemInSession=21, lastName='Harrell', length=241.34485, level='paid', location='Lansing-East Lansing, MI', method='PUT', page='NextSong', registration=1540472624796.0, sessionId=605, song='Safe 2 Say [The Incredible] (Album Version - Amended)', status=200, ts=1542296032796, userAgent='\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"', userId='97'),\n",
       " Row(artist='Linkin Park', auth='Logged In', firstName='Kate', gender='F', itemInSession=33, lastName='Harrell', length=259.86567, level='paid', location='Lansing-East Lansing, MI', method='PUT', page='NextSong', registration=1540472624796.0, sessionId=605, song='My December', status=200, ts=1542299023796, userAgent='\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"', userId='97'),\n",
       " Row(artist='The Saturdays', auth='Logged In', firstName='Chloe', gender='F', itemInSession=20, lastName='Cuevas', length=176.95302, level='paid', location='San Francisco-Oakland-Hayward, CA', method='PUT', page='NextSong', registration=1540940782796.0, sessionId=630, song='If This Is Love', status=200, ts=1542318319796, userAgent='Mozilla/5.0 (Windows NT 5.1; rv:31.0) Gecko/20100101 Firefox/31.0', userId='49'),\n",
       " Row(artist='Wim Mertens', auth='Logged In', firstName='Aleena', gender='F', itemInSession=71, lastName='Kirby', length=240.79628, level='paid', location='Waterloo-Cedar Falls, IA', method='PUT', page='NextSong', registration=1541022995796.0, sessionId=619, song='Naviamente', status=200, ts=1542321121796, userAgent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Firefox/31.0', userId='44'),\n",
       " Row(artist=None, auth='Logged In', firstName='Tegan', gender='F', itemInSession=26, lastName='Levine', length=None, level='paid', location='Portland-South Portland, ME', method='GET', page='Home', registration=1540794356796.0, sessionId=774, song=None, status=200, ts=1542768175796, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='80')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing first few rows of log_df\n",
    "log_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering by 'Next Song' action\n",
    "log_df = log_df.filter(log_df['page'] == 'NextSong')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Table\n",
    "We'll peel off the columns needed for this specific table and write them to the appropriate parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peeling off appripriate columns from log_df for the users_table\n",
    "users_table = log_df['userId', 'firstName', 'lastName', 'gender', 'level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates from the table\n",
    "users_table = users_table.drop_duplicates(subset=['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the users_table information to the appropriate parquet file\n",
    "users_table.write.parquet(output_data + 'users/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Table\n",
    "This table is going to be a bit tricky because while we are given the timestamp, we are not provided with the individual fields called for by the project. No matter, we'll simply extract this appropriate information from the timestamp ('ts') field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a UDF to extract the information needed appropriately\n",
    "get_datetime = udf(lambda x: datetime.fromtimestamp(x / 1000.0).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new 'start_date' column using UDF defined above\n",
    "log_df = log_df.withColumn('start_date', get_datetime(log_df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding month to log_df for later use\n",
    "log_df = log_df.withColumn('month', month(log_df.start_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating time_table using PySpark SQL functions to extract information appropriately\n",
    "time_table = log_df.select('start_date',\n",
    "                           hour('start_date').alias('hour'),\n",
    "                           dayofmonth('start_date').alias('day'),\n",
    "                           weekofyear('start_date').alias('week'),\n",
    "                           month('start_date').alias('month'),\n",
    "                           year('start_date').alias('year'),\n",
    "                           dayofweek('start_date').alias('weekday')\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the time_table information to the appropriate parquet file and partitioning by year/month\n",
    "time_table.write.partitionBy('year', 'month').parquet(output_data + 'time/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Songplays Table\n",
    "Being the fact table, creating this table will be a tad tricky as we'll need to leverage some of the data we already worked with above. Let's first read back in those parquet files we established above for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in parquet file from above\n",
    "songs_table_df = spark.read.parquet(output_data + '/songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking schema of newly written-back-in songs_table_df\n",
    "songs_table_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining all the tables together\n",
    "log_df = log_df.join(songs_table_df, (log_df.song == songs_table_df.title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating baseline songplay_table WITHOUT songplay_id\n",
    "songplay_table = log_df['start_date', 'userId', 'level', 'song_id', 'artist_id', 'location', 'userAgent', 'year', 'month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Special thanks to this post for helping me figure out how to add songplay_id column: https://stackoverflow.com/questions/32086578/how-to-add-row-id-in-pyspark-dataframes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding songplay_id column\n",
    "songplay_table = songplay_table.withColumn('songplay_id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing songplay_table to appropriate parquet file, partitioning by year/month\n",
    "songplay_table.write.partitionBy('year', 'month').parquet(output_data + 'songplay/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
